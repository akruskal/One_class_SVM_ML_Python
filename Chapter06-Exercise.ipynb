{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/HDAT9500Banner.PNG)\n",
    "<br>\n",
    "\n",
    "# Chapter 6: Artificial Neural Networks / Deep Learning\n",
    "# Exercise 01: \n",
    "\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "In this exercise, \n",
    "\n",
    "\n",
    "## 1.1. Aims of the Exercise:\n",
    "\n",
    "1. This is an introduction to Artificial Neural Networks / Deep Learning. \n",
    "2. We will use Keras, a high-level API built on top of low level neural networks APIs such as Tensorflow and Theano. Keras takes care of many things and it is easy to use.\n",
    "\n",
    " \n",
    "It aligns with all of the learning outcomes of our course: \n",
    "\n",
    "1.\tDistinguish a range of task specific machine learning techniques appropriate for Health Data Science.\n",
    "2.\tDesign machine learning tasks for Health Data Science scenarios.\n",
    "3.\tConstruct appropriate training and test sets for health research data.\n",
    "\n",
    "\n",
    "## 1.2. Jupyter Notebook Intructions\n",
    "1. Read the content of each cell.\n",
    "2. Where necessary, follow the instructions that are written in each cell.\n",
    "3. Run/Execute all the cells that contain Python code sequentially (one at a time), using the \"Run\" button.\n",
    "4. For those cells in which you are asked to write some code, please write the Python code first and then execute/run the cell.\n",
    " \n",
    "## 1.3. Tips\n",
    " 1. The square brackets on the left hand side of each cell indicate whether the cell has been executed or not. Empty square brackets mean that the cell has not been executed, whereas square brackets that contain a number means that the cell has been executed. Run all the cells in sequence, using the \"Run\" button.\n",
    " 2. To edit this notebook, just double-click in each cell. In the document, each cell can be a \"Code\" cell or \"text-Markdown\" cell. To choose between these two options, go to the combo-box above. \n",
    " 3. If you want to save your notebook, please make sure you press the \"floppy disk\" icon button above. \n",
    " 4. To clean the content of all cells and re-start the Notebook, please go to Cell->All Output->Clear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the Wisconsin Cancer Data Set and Prepare the data\n",
    "\n",
    "For data dictionary and all information:\n",
    "https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "#For this notebook to work, Python must be 3.6.4 or 3.6.5\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from plotnine import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = pd.read_csv('data/breast-cancer-wisconsin-data/data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "            ...             radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           ...                    25.38          17.33           184.60   \n",
       "1           ...                    24.99          23.41           158.80   \n",
       "2           ...                    23.57          25.53           152.50   \n",
       "3           ...                    14.91          26.50            98.87   \n",
       "4           ...                    22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 32)\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check:\n",
    "display(cancer[:][:5])\n",
    "print(cancer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                           int64\n",
       "diagnosis                   object\n",
       "radius_mean                float64\n",
       "texture_mean               float64\n",
       "perimeter_mean             float64\n",
       "area_mean                  float64\n",
       "smoothness_mean            float64\n",
       "compactness_mean           float64\n",
       "concavity_mean             float64\n",
       "concave points_mean        float64\n",
       "symmetry_mean              float64\n",
       "fractal_dimension_mean     float64\n",
       "radius_se                  float64\n",
       "texture_se                 float64\n",
       "perimeter_se               float64\n",
       "area_se                    float64\n",
       "smoothness_se              float64\n",
       "compactness_se             float64\n",
       "concavity_se               float64\n",
       "concave points_se          float64\n",
       "symmetry_se                float64\n",
       "fractal_dimension_se       float64\n",
       "radius_worst               float64\n",
       "texture_worst              float64\n",
       "perimeter_worst            float64\n",
       "area_worst                 float64\n",
       "smoothness_worst           float64\n",
       "compactness_worst          float64\n",
       "concavity_worst            float64\n",
       "concave points_worst       float64\n",
       "symmetry_worst             float64\n",
       "fractal_dimension_worst    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into X and y (output, labels)\n",
    "X = cancer.iloc[:, 2:].values\n",
    "y_categorical = cancer.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.79900000e+01,   1.03800000e+01,   1.22800000e+02,\n",
       "          1.00100000e+03,   1.18400000e-01,   2.77600000e-01,\n",
       "          3.00100000e-01,   1.47100000e-01,   2.41900000e-01,\n",
       "          7.87100000e-02,   1.09500000e+00,   9.05300000e-01,\n",
       "          8.58900000e+00,   1.53400000e+02,   6.39900000e-03,\n",
       "          4.90400000e-02,   5.37300000e-02,   1.58700000e-02,\n",
       "          3.00300000e-02,   6.19300000e-03,   2.53800000e+01,\n",
       "          1.73300000e+01,   1.84600000e+02,   2.01900000e+03,\n",
       "          1.62200000e-01,   6.65600000e-01,   7.11900000e-01,\n",
       "          2.65400000e-01,   4.60100000e-01,   1.18900000e-01],\n",
       "       [  2.05700000e+01,   1.77700000e+01,   1.32900000e+02,\n",
       "          1.32600000e+03,   8.47400000e-02,   7.86400000e-02,\n",
       "          8.69000000e-02,   7.01700000e-02,   1.81200000e-01,\n",
       "          5.66700000e-02,   5.43500000e-01,   7.33900000e-01,\n",
       "          3.39800000e+00,   7.40800000e+01,   5.22500000e-03,\n",
       "          1.30800000e-02,   1.86000000e-02,   1.34000000e-02,\n",
       "          1.38900000e-02,   3.53200000e-03,   2.49900000e+01,\n",
       "          2.34100000e+01,   1.58800000e+02,   1.95600000e+03,\n",
       "          1.23800000e-01,   1.86600000e-01,   2.41600000e-01,\n",
       "          1.86000000e-01,   2.75000000e-01,   8.90200000e-02],\n",
       "       [  1.96900000e+01,   2.12500000e+01,   1.30000000e+02,\n",
       "          1.20300000e+03,   1.09600000e-01,   1.59900000e-01,\n",
       "          1.97400000e-01,   1.27900000e-01,   2.06900000e-01,\n",
       "          5.99900000e-02,   7.45600000e-01,   7.86900000e-01,\n",
       "          4.58500000e+00,   9.40300000e+01,   6.15000000e-03,\n",
       "          4.00600000e-02,   3.83200000e-02,   2.05800000e-02,\n",
       "          2.25000000e-02,   4.57100000e-03,   2.35700000e+01,\n",
       "          2.55300000e+01,   1.52500000e+02,   1.70900000e+03,\n",
       "          1.44400000e-01,   4.24500000e-01,   4.50400000e-01,\n",
       "          2.43000000e-01,   3.61300000e-01,   8.75800000e-02],\n",
       "       [  1.14200000e+01,   2.03800000e+01,   7.75800000e+01,\n",
       "          3.86100000e+02,   1.42500000e-01,   2.83900000e-01,\n",
       "          2.41400000e-01,   1.05200000e-01,   2.59700000e-01,\n",
       "          9.74400000e-02,   4.95600000e-01,   1.15600000e+00,\n",
       "          3.44500000e+00,   2.72300000e+01,   9.11000000e-03,\n",
       "          7.45800000e-02,   5.66100000e-02,   1.86700000e-02,\n",
       "          5.96300000e-02,   9.20800000e-03,   1.49100000e+01,\n",
       "          2.65000000e+01,   9.88700000e+01,   5.67700000e+02,\n",
       "          2.09800000e-01,   8.66300000e-01,   6.86900000e-01,\n",
       "          2.57500000e-01,   6.63800000e-01,   1.73000000e-01],\n",
       "       [  2.02900000e+01,   1.43400000e+01,   1.35100000e+02,\n",
       "          1.29700000e+03,   1.00300000e-01,   1.32800000e-01,\n",
       "          1.98000000e-01,   1.04300000e-01,   1.80900000e-01,\n",
       "          5.88300000e-02,   7.57200000e-01,   7.81300000e-01,\n",
       "          5.43800000e+00,   9.44400000e+01,   1.14900000e-02,\n",
       "          2.46100000e-02,   5.68800000e-02,   1.88500000e-02,\n",
       "          1.75600000e-02,   5.11500000e-03,   2.25400000e+01,\n",
       "          1.66700000e+01,   1.52200000e+02,   1.57500000e+03,\n",
       "          1.37400000e-01,   2.05000000e-01,   4.00000000e-01,\n",
       "          1.62500000e-01,   2.36400000e-01,   7.67800000e-02]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sanity check\n",
    "display(X[:][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to the shape of the input vector!!!!\n",
    "We will use it later in our ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n"
     ]
    }
   ],
   "source": [
    " print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 1:  What is the meaning of (569,30) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Write answer here:</b>\n",
    "#####################################################################################################################\n",
    "\n",
    "(Double-click here)\n",
    "\n",
    "\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "       'M', 'M', 'M', 'M', 'M', 'M', 'B', 'B', 'B', 'M', 'M', 'M', 'M',\n",
       "       'M', 'M', 'M', 'M'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check:\n",
    "# We can see that # 19, 20 and 21 are B within the array that goes from 0:29\n",
    "display(y_categorical[:30])\n",
    "print(y_categorical.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enconding categorical data into 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "y = labelencoder_X_1.fit_transform(y_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "# We can see that # 19, 20 and 21 are \"0\" within the array that goes from 0:29\n",
    "# Therefore, the sanity check confirms that we have done the encoding correctly\n",
    "display(y[:30])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Scaling</font>  our data is <font color=red> very important </font> when we use ANNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very very very important: Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, everything is very familiar. We have used some new scikit learn instructions, but esentially, we have been following these steps during the course.\n",
    "\n",
    "The new part starts here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Our first ANN using Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 2: Initalise ANN: Check page 28 of the book we use in this chapter and write the command to initialise our first ANN </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python Code here:\n",
    "ann1  = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 3:  Add input layer and first hidden layer: Using the function add, create the input layer and fist hidden layer with 16 nodes, a relu activation function. Use the argument input_dim </font>\n",
    "<p>\n",
    "<font color='green'>Page 28 of our book</font>\n",
    "<p>\n",
    "<font color='green'>Read here how to use the function \"add\" by using the argument input_dim: \n",
    "<p>\n",
    "    https://keras.io/getting-started/sequential-model-guide/</font>\n",
    "<p>\n",
    "<font color='green'> In addition, read:  \n",
    "     https://keras.io/layers/merge/#add_1</font>\n",
    "<p>\n",
    "<font color='green'> Function \"dense\":           \n",
    "    https://keras.io/layers/core/#dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z3368601\\AppData\\Local\\Anacondav3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=30, units=16)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Write Python Code here:\n",
    "ann1.add(Dense(output_dim=16, input_dim=30))\n",
    "ann1.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 4:  Add second hidden layer: Using the functions \"add\" and \"dense\", create a second layer with 16 nodes, a relu activation function.  </font>\n",
    "<p>\n",
    "<font color='green'>Page 28 of our book</font>\n",
    "<p>\n",
    "<font color='green'>Read here how to use the function \"add\" by using the argument input_dim: \n",
    "<p>\n",
    "    https://keras.io/getting-started/sequential-model-guide/</font>\n",
    "<p>\n",
    "<font color='green'> In addition, read:  \n",
    "     https://keras.io/layers/merge/#add_1</font>\n",
    "<p>\n",
    "<font color='green'> Function \"dense\":           \n",
    "    https://keras.io/layers/core/#dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z3368601\\AppData\\Local\\Anacondav3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=16)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Write Python Code here:\n",
    "ann1.add(Dense(output_dim=16, activation='relu'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 5:  Add output layer: Using the functions \"add\" and \"dense\". What activation function would you use and why?  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z3368601\\AppData\\Local\\Anacondav3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Write Python Code here:\n",
    "ann1.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 6: Compile our model.  Use gradient descent (\"adam\", for example) for the optimizer, \n",
    "\"binary_crossentropy\" as the loss function, and accuracy as our metric.  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python Code here:\n",
    "# Compiling the ANN\n",
    "ann1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 7: # Fitting the ANN to the Training set. Set the batch_size=100, nb_epoch=150. These hyper-parameters have to be tuned. These numbers have been optimised already. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z3368601\\AppData\\Local\\Anacondav3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "512/512 [==============================] - 0s 943us/step - loss: 0.6928 - acc: 0.6328\n",
      "Epoch 2/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.6916 - acc: 0.6289\n",
      "Epoch 3/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.6900 - acc: 0.6289\n",
      "Epoch 4/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.6879 - acc: 0.6289\n",
      "Epoch 5/150\n",
      "512/512 [==============================] - 0s 21us/step - loss: 0.6847 - acc: 0.6289\n",
      "Epoch 6/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.6799 - acc: 0.6289\n",
      "Epoch 7/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.6718 - acc: 0.6289\n",
      "Epoch 8/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.6586 - acc: 0.6289\n",
      "Epoch 9/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.6376 - acc: 0.6328\n",
      "Epoch 10/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.6033 - acc: 0.7012\n",
      "Epoch 11/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.5538 - acc: 0.8555\n",
      "Epoch 12/150\n",
      "512/512 [==============================] - 0s 29us/step - loss: 0.4881 - acc: 0.9238\n",
      "Epoch 13/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.4148 - acc: 0.9512\n",
      "Epoch 14/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.3448 - acc: 0.9512\n",
      "Epoch 15/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.2827 - acc: 0.9551\n",
      "Epoch 16/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.2281 - acc: 0.9609\n",
      "Epoch 17/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.1793 - acc: 0.9629\n",
      "Epoch 18/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.1408 - acc: 0.9668\n",
      "Epoch 19/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.1165 - acc: 0.9687\n",
      "Epoch 20/150\n",
      "512/512 [==============================] - 0s 18us/step - loss: 0.1015 - acc: 0.9707\n",
      "Epoch 21/150\n",
      "512/512 [==============================] - 0s 29us/step - loss: 0.0907 - acc: 0.9727\n",
      "Epoch 22/150\n",
      "512/512 [==============================] - 0s 21us/step - loss: 0.0843 - acc: 0.9766\n",
      "Epoch 23/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0796 - acc: 0.9805\n",
      "Epoch 24/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0763 - acc: 0.9824\n",
      "Epoch 25/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.0731 - acc: 0.9844\n",
      "Epoch 26/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0708 - acc: 0.9824\n",
      "Epoch 27/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0688 - acc: 0.9824\n",
      "Epoch 28/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.0672 - acc: 0.9824\n",
      "Epoch 29/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0658 - acc: 0.9844\n",
      "Epoch 30/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0645 - acc: 0.9844\n",
      "Epoch 31/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0636 - acc: 0.9844\n",
      "Epoch 32/150\n",
      "512/512 [==============================] - 0s 21us/step - loss: 0.0627 - acc: 0.9863\n",
      "Epoch 33/150\n",
      "512/512 [==============================] - 0s 18us/step - loss: 0.0617 - acc: 0.9824\n",
      "Epoch 34/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0604 - acc: 0.9844\n",
      "Epoch 35/150\n",
      "512/512 [==============================] - 0s 18us/step - loss: 0.0596 - acc: 0.9844\n",
      "Epoch 36/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.0586 - acc: 0.9844\n",
      "Epoch 37/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0581 - acc: 0.9844\n",
      "Epoch 38/150\n",
      "512/512 [==============================] - 0s 18us/step - loss: 0.0573 - acc: 0.9844\n",
      "Epoch 39/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0564 - acc: 0.9863\n",
      "Epoch 40/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0557 - acc: 0.9863\n",
      "Epoch 41/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.0552 - acc: 0.9863\n",
      "Epoch 42/150\n",
      "512/512 [==============================] - 0s 21us/step - loss: 0.0546 - acc: 0.9844\n",
      "Epoch 43/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0537 - acc: 0.9863\n",
      "Epoch 44/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0534 - acc: 0.9883\n",
      "Epoch 45/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0528 - acc: 0.9883\n",
      "Epoch 46/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0525 - acc: 0.9883\n",
      "Epoch 47/150\n",
      "512/512 [==============================] - 0s 18us/step - loss: 0.0519 - acc: 0.9883\n",
      "Epoch 48/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0511 - acc: 0.9883\n",
      "Epoch 49/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0505 - acc: 0.9883\n",
      "Epoch 50/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0499 - acc: 0.9883\n",
      "Epoch 51/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0493 - acc: 0.9883\n",
      "Epoch 52/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.0489 - acc: 0.9883\n",
      "Epoch 53/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.0485 - acc: 0.9883\n",
      "Epoch 54/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0480 - acc: 0.9883\n",
      "Epoch 55/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.0475 - acc: 0.9883\n",
      "Epoch 56/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0465 - acc: 0.9863\n",
      "Epoch 57/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.0470 - acc: 0.9863\n",
      "Epoch 58/150\n",
      "512/512 [==============================] - 0s 33us/step - loss: 0.0464 - acc: 0.9863\n",
      "Epoch 59/150\n",
      "512/512 [==============================] - 0s 21us/step - loss: 0.0454 - acc: 0.9883\n",
      "Epoch 60/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0451 - acc: 0.9883\n",
      "Epoch 61/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0449 - acc: 0.9883\n",
      "Epoch 62/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0444 - acc: 0.9883\n",
      "Epoch 63/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0435 - acc: 0.9883\n",
      "Epoch 64/150\n",
      "512/512 [==============================] - 0s 21us/step - loss: 0.0429 - acc: 0.9883\n",
      "Epoch 65/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0424 - acc: 0.9883\n",
      "Epoch 66/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0418 - acc: 0.9883\n",
      "Epoch 67/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0414 - acc: 0.9883\n",
      "Epoch 68/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.0408 - acc: 0.9902\n",
      "Epoch 69/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0409 - acc: 0.9902\n",
      "Epoch 70/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0400 - acc: 0.9902\n",
      "Epoch 71/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.0397 - acc: 0.9922\n",
      "Epoch 72/150\n",
      "512/512 [==============================] - 0s 18us/step - loss: 0.0408 - acc: 0.9902\n",
      "Epoch 73/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0401 - acc: 0.9902\n",
      "Epoch 74/150\n",
      "512/512 [==============================] - 0s 21us/step - loss: 0.0385 - acc: 0.9902\n",
      "Epoch 75/150\n",
      "512/512 [==============================] - 0s 16us/step - loss: 0.0376 - acc: 0.9922\n",
      "Epoch 76/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.0374 - acc: 0.9922\n",
      "Epoch 77/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0375 - acc: 0.9922\n",
      "Epoch 78/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0372 - acc: 0.9922\n",
      "Epoch 79/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.0366 - acc: 0.9922\n",
      "Epoch 80/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0363 - acc: 0.9922\n",
      "Epoch 81/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0359 - acc: 0.9922\n",
      "Epoch 82/150\n",
      "512/512 [==============================] - 0s 21us/step - loss: 0.0356 - acc: 0.9922\n",
      "Epoch 83/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0351 - acc: 0.9922\n",
      "Epoch 84/150\n",
      "512/512 [==============================] - 0s 18us/step - loss: 0.0356 - acc: 0.9922\n",
      "Epoch 85/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0351 - acc: 0.9922\n",
      "Epoch 86/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0345 - acc: 0.9922\n",
      "Epoch 87/150\n",
      "512/512 [==============================] - 0s 18us/step - loss: 0.0342 - acc: 0.9922\n",
      "Epoch 88/150\n",
      "512/512 [==============================] - 0s 18us/step - loss: 0.0340 - acc: 0.9922\n",
      "Epoch 89/150\n",
      "512/512 [==============================] - 0s 18us/step - loss: 0.0337 - acc: 0.9922\n",
      "Epoch 90/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.0335 - acc: 0.9922\n",
      "Epoch 91/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0332 - acc: 0.9922\n",
      "Epoch 92/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0333 - acc: 0.9922\n",
      "Epoch 93/150\n",
      "512/512 [==============================] - 0s 21us/step - loss: 0.0330 - acc: 0.9922\n",
      "Epoch 94/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.0327 - acc: 0.9922\n",
      "Epoch 95/150\n",
      "512/512 [==============================] - 0s 31us/step - loss: 0.0323 - acc: 0.9922\n",
      "Epoch 96/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0320 - acc: 0.9922\n",
      "Epoch 97/150\n",
      "512/512 [==============================] - 0s 29us/step - loss: 0.0319 - acc: 0.9922\n",
      "Epoch 98/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0316 - acc: 0.9922\n",
      "Epoch 99/150\n",
      "512/512 [==============================] - 0s 21us/step - loss: 0.0313 - acc: 0.9922\n",
      "Epoch 100/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.0311 - acc: 0.9922\n",
      "Epoch 101/150\n",
      "512/512 [==============================] - 0s 21us/step - loss: 0.0308 - acc: 0.9922\n",
      "Epoch 102/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0306 - acc: 0.9922\n",
      "Epoch 103/150\n",
      "512/512 [==============================] - 0s 21us/step - loss: 0.0305 - acc: 0.9922\n",
      "Epoch 104/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0302 - acc: 0.9922\n",
      "Epoch 105/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0298 - acc: 0.9922\n",
      "Epoch 106/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.0296 - acc: 0.9922\n",
      "Epoch 107/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0289 - acc: 0.9922\n",
      "Epoch 108/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0283 - acc: 0.9922\n",
      "Epoch 109/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0283 - acc: 0.9922\n",
      "Epoch 110/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.0277 - acc: 0.9922\n",
      "Epoch 111/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0268 - acc: 0.9922\n",
      "Epoch 112/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0256 - acc: 0.9922\n",
      "Epoch 113/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0248 - acc: 0.9922\n",
      "Epoch 114/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0241 - acc: 0.9922\n",
      "Epoch 115/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0233 - acc: 0.9941\n",
      "Epoch 116/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.0226 - acc: 0.9941\n",
      "Epoch 117/150\n",
      "512/512 [==============================] - 0s 21us/step - loss: 0.0248 - acc: 0.9941\n",
      "Epoch 118/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0234 - acc: 0.9961\n",
      "Epoch 119/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0221 - acc: 0.9941\n",
      "Epoch 120/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0221 - acc: 0.9941\n",
      "Epoch 121/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0214 - acc: 0.9941\n",
      "Epoch 122/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0206 - acc: 0.9961\n",
      "Epoch 123/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0202 - acc: 0.9961\n",
      "Epoch 124/150\n",
      "512/512 [==============================] - 0s 29us/step - loss: 0.0199 - acc: 0.9961\n",
      "Epoch 125/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0197 - acc: 0.9961\n",
      "Epoch 126/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0194 - acc: 0.9961\n",
      "Epoch 127/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0192 - acc: 0.9961\n",
      "Epoch 128/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0190 - acc: 0.9961\n",
      "Epoch 129/150\n",
      "512/512 [==============================] - 0s 21us/step - loss: 0.0188 - acc: 0.9961\n",
      "Epoch 130/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0186 - acc: 0.9961\n",
      "Epoch 131/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0184 - acc: 0.9961\n",
      "Epoch 132/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0182 - acc: 0.9961\n",
      "Epoch 133/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0180 - acc: 0.9961\n",
      "Epoch 134/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0178 - acc: 0.9961\n",
      "Epoch 135/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0176 - acc: 0.9961\n",
      "Epoch 136/150\n",
      "512/512 [==============================] - 0s 29us/step - loss: 0.0174 - acc: 0.9961\n",
      "Epoch 137/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0173 - acc: 0.9961\n",
      "Epoch 138/150\n",
      "512/512 [==============================] - 0s 23us/step - loss: 0.0172 - acc: 0.9961\n",
      "Epoch 139/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.0170 - acc: 0.9961\n",
      "Epoch 140/150\n",
      "512/512 [==============================] - 0s 21us/step - loss: 0.0168 - acc: 0.9961\n",
      "Epoch 141/150\n",
      "512/512 [==============================] - 0s 29us/step - loss: 0.0164 - acc: 0.9961\n",
      "Epoch 142/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0163 - acc: 0.9961\n",
      "Epoch 143/150\n",
      "512/512 [==============================] - 0s 20us/step - loss: 0.0161 - acc: 0.9961\n",
      "Epoch 144/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0160 - acc: 0.9961\n",
      "Epoch 145/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0160 - acc: 0.9961\n",
      "Epoch 146/150\n",
      "512/512 [==============================] - 0s 21us/step - loss: 0.0156 - acc: 0.9961\n",
      "Epoch 147/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0154 - acc: 0.9961\n",
      "Epoch 148/150\n",
      "512/512 [==============================] - 0s 25us/step - loss: 0.0153 - acc: 0.9961\n",
      "Epoch 149/150\n",
      "512/512 [==============================] - 0s 18us/step - loss: 0.0151 - acc: 0.9961\n",
      "Epoch 150/150\n",
      "512/512 [==============================] - 0s 27us/step - loss: 0.0148 - acc: 0.9961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10204a20>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write Python Code here:\n",
    "ann1.fit(X_train, y_train, batch_size=100, nb_epoch=150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 8: Calculate accuracy, confusion matrix and all the metrics included in classification_report function  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python Code here:\n",
    "# Compiling the ANN\n",
    "# Predicting the Test set results\n",
    "y_pred = ann1.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our accuracy is 98.24561403508771%\n"
     ]
    }
   ],
   "source": [
    "print(\"Our accuracy is {}%\".format(((cm[0][0] + cm[1][1])/57)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cm,annot=True)\n",
    "plt.savefig('h.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity**</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
